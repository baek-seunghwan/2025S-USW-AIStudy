# 인터뷰 준비 노트 – 보험 사기 탐지 프로젝트

본 문서는 **2025 수원대학교 AI 스터디 대회(Dacon ML2 트랙)** 보험 사기 탐지 과제에서의 핵심 경험과 학습 내용을 정리한 것입니다. 새로운 기술을 어떻게 채택·적용했는지, 프로젝트를 어떻게 리드했는지, 복잡한 데이터 구조를 어떻게 이해했는지를 강조합니다.

## 1. 새로운 기술 학습 및 적용

### 데이터 분석·머신러닝을 위한 Python

**Python**을 주 언어로 사용해 데이터 탐색, 특성 공학, 모델 개발을 수행했습니다. `pandas`, `NumPy`로 정제·변환을 진행했고, `scikit-learn`으로 베이스라인 모델과 평가 지표를 검증했습니다. 또한 **CatBoost**, **LightGBM**, **XGBoost**와 같은 그래디언트 부스팅 프레임워크를 학습·적용해 단순 알고리즘 대비 더 정확한 사기 탐지를 달성했습니다.

### 확장 가능한 처리의 Apache Spark

프로토타이핑 단계에서 특성 공학이 복잡해지며 로컬 처리 한계를 인지했습니다. 대용량·고차원 실험을 지원하기 위해 **Apache Spark**를 독학하여 **PySpark** 기반으로 ETL 일부를 재작성했습니다. 분산 DataFrame 연산으로 수백만 레코드를 멀티코어 메모리 상에서 처리해 전처리 시간을 **약 한 자릿수 배** 단축했고, 단일 머신에서 어려웠던 고차원 특성 실험을 가능하게 했습니다.

### 파이프라인 오케스트레이션을 위한 Airflow

수작업으로 데이터 수집·특성 공학·모델 학습·제출을 실행하는 과정은 오류와 시간 소모가 컸습니다. 이를 자동화하기 위해 **Apache Airflow**를 학습하고 end-to-end 워크플로우용 DAG를 구축했습니다. 신규 데이터 감지를 위한 센서, Spark 잡 실행, CatBoost 학습, 최종 제출 파일 생성 태스크로 구성했으며, 스케줄링·재시도 메커니즘을 통해 신뢰성을 높이고 수작업을 줄여 모델 개선에 집중할 수 있었습니다.

### 빅데이터 저장을 위한 Hadoop

대회 범위를 넘어선 이력 청구·트랜잭션 로그 보관을 위해 소규모 **Hadoop** 클러스터를 구성했습니다. 원천 데이터를 **HDFS**에 적재하고 **Hive**로 노출하여 Spark에서 직접 조회·샘플링 가능하게 했습니다. 로컬 디스크 저장 한계를 해소하고, 다중 데이터 소스 조인 성능을 개선했습니다. 선택 기준은 Spark와의 통합 용이성과 내결함성 아키텍처였습니다.

## 2. 프로젝트 기여와 성과

사기 탐지 모델링 전 과정을 리드했습니다. 여러 알고리즘을 탐색한 결과, 튜닝된 **CatBoost** 분류기가 매크로 F1에서 가장 우수했습니다. 핵심 기여와 성과는 다음과 같습니다.

* **특성 공학**: 월·일 문자열을 수치형으로 변환하고 `payout_income_ratio`, `driver_vehicle_age_ratio`, `driver_vehicle_age_diff`, `liab_payout`, `past_claims_ratio_income` 등 비율·차이 특성을 설계했습니다. 도메인 지식을 반영해 판별력을 개선했습니다.
* **교차검증·하이퍼파라미터 튜닝**: 층화 폴드를 활용해 검증하고 CatBoost의 반복 수, 깊이, 학습률, 클래스 가중치 등을 최적화해 베이스라인 **~0.20**이던 매크로 F1을 **0.60+**까지 끌어올렸습니다(퍼블릭 리더보드 기준).
* **임계값 최적화**: 매크로 F1은 확률 임계값의 영향을 받는다는 점에 착안해 학습 예측을 분석, **0.56–0.60** 구간에서 정밀도·재현율 균형을 맞춰 퍼블릭 리더보드 **2위** 점수를 달성했습니다.
* **자동화·재현성**: 전체 파이프라인을 Airflow로 구현해 수동 실행 시간을 **50%+** 절감, 실험 간 결과 일관성을 확보했습니다.
* **대회 성과**: 최종 제출(`ML2_submission_catboost_tuned2_thr1300.csv`)은 매크로 F1 **0.602+**를 기록하며 제출 시점 기준 **2위**를 달성했습니다.

## 3. 데이터 구조에 대한 심층 이해

보험 청구 데이터는 수치·범주 혼합 변수, 결측치, 파생 특성 처리가 핵심이었습니다. 저는 다음을 수행했습니다.

* **범주형 처리**: 순서형 인코딩을 적용하고 변수 특성에 맞춘 적절한 결측치 대체 전략을 사용했습니다.
* **의미 있는 파생 변수 설계**: 소득 대비 보상금, 운전자·차량 연령 간 관계 등 실제 비즈니스 로직을 반영한 비율·차이 특성으로 모델의 표현력을 강화했습니다.
* **복합 포맷 처리**: 다른 프로젝트에서는 Spark·Hadoop으로 중첩 JSON, 반정형 클릭스트림 등을 다루며 중첩 필드 평탄화, 대용량 파티셔닝, 다중 소스 조인 시 참조 무결성 보장을 구현했습니다.
* **도메인 검증**: 책임 비율, 과거 청구 횟수 등 도메인 개념을 조사·검증해 파생 특성의 비즈니스 타당성을 확보했습니다.

이러한 경험을 통해 새로운 기술을 빠르게 학습·적용하고, 실무 문제를 해결하며, 프로젝트를 정량 성과로 이끌고, 복잡한 데이터 구조를 논리적으로 해석하는 역량을 증명했습니다.
